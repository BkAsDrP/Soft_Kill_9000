{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b6c9e5",
   "metadata": {},
   "source": [
    "# SOFTKILL-9000: Multi-Agent Motion Capture Simulation\n",
    "\n",
    "This notebook demonstrates how to run SOFTKILL-9000 in Google Colab.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/BkAsDrP/Softkill9000/blob/main/examples/run_in_colab.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "SOFTKILL-9000 is a multi-agent reinforcement learning system for motion capture simulation. This notebook will:\n",
    "\n",
    "1. Install the package from GitHub\n",
    "2. Run a basic simulation\n",
    "3. Visualize agent trajectories\n",
    "4. Demonstrate Q-learning training\n",
    "5. Show advanced customization options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7687a26b",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "Install SOFTKILL-9000 directly from the GitHub repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b50f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package from GitHub\n",
    "!pip install git+https://github.com/BkAsDrP/Softkill9000.git -q\n",
    "\n",
    "# Verify installation\n",
    "import softkill9000\n",
    "print(f\"âœ… SOFTKILL-9000 v{softkill9000.__version__} installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b96739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade NumPy to resolve dependency conflicts\n",
    "# Colab comes with NumPy 1.x, but softkill9000 needs NumPy 2.x for modern ML libraries\n",
    "# (jax, opencv, pytensor, thinc all require numpy>=2.0)\n",
    "!pip install --upgrade \"numpy>=2.0.0,<3.0.0\" -q\n",
    "\n",
    "# Verify NumPy version\n",
    "import numpy as np\n",
    "print(f\"âœ… NumPy {np.__version__} ready (compatible with JAX, OpenCV, PyTensor)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ec9d7b",
   "metadata": {},
   "source": [
    "## 2. Basic Simulation\n",
    "\n",
    "Let's run a basic simulation with default configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59852e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from softkill9000.simulator import MissionSimulator\n",
    "from softkill9000.agents.agent import Agent\n",
    "from softkill9000.environments.environment import CosmicScenario\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create agents\n",
    "agents = [\n",
    "    Agent(agent_id=\"Longsight\", role=\"scout\"),\n",
    "    Agent(agent_id=\"Lifebinder\", role=\"medic\"),\n",
    "    Agent(agent_id=\"Specter\", role=\"assault\")\n",
    "]\n",
    "\n",
    "# Create scenario\n",
    "scenario = CosmicScenario.generate_random(\n",
    "    width=100.0,\n",
    "    height=100.0,\n",
    "    num_objectives=3,\n",
    "    num_obstacles=5\n",
    ")\n",
    "\n",
    "# Initialize simulator\n",
    "simulator = MissionSimulator(\n",
    "    agents=agents,\n",
    "    scenario=scenario,\n",
    "    timesteps=20\n",
    ")\n",
    "\n",
    "# Run simulation\n",
    "print(\"ðŸš€ Starting simulation...\\n\")\n",
    "results = simulator.run_simulation()\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š SIMULATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Reward: {results['total_reward']:.2f}\")\n",
    "print(f\"Objectives Completed: {results['objectives_completed']}\")\n",
    "print(f\"Average Health: {results['average_health']:.1f}%\")\n",
    "print(f\"Final Morale: {results['final_morale']:.1f}%\")\n",
    "print(f\"Completion Time: {results['completion_time']:.2f}s\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1804a461",
   "metadata": {},
   "source": [
    "## 3. Visualize Agent Trajectories\n",
    "\n",
    "Plot the paths taken by each agent during the mission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d4b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from softkill9000.visualization import plot_agent_trajectories\n",
    "\n",
    "# Plot trajectories\n",
    "fig = plot_agent_trajectories(results['agent_histories'], scenario)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“ Agent Movement Summary:\")\n",
    "for agent_id, history in results['agent_histories'].items():\n",
    "    positions = [(h['position']['x'], h['position']['y']) for h in history]\n",
    "    start = positions[0]\n",
    "    end = positions[-1]\n",
    "    distance = sum(\n",
    "        np.sqrt((positions[i+1][0] - positions[i][0])**2 + \n",
    "                (positions[i+1][1] - positions[i][1])**2)\n",
    "        for i in range(len(positions)-1)\n",
    "    )\n",
    "    print(f\"  {agent_id}: {distance:.1f} units traveled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbd28fe",
   "metadata": {},
   "source": [
    "## 4. Performance Metrics Over Time\n",
    "\n",
    "Visualize how agent stats evolved during the simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31ffac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from softkill9000.visualization import plot_performance_metrics\n",
    "\n",
    "# Plot performance metrics\n",
    "fig = plot_performance_metrics(results['agent_histories'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“ˆ Performance Analysis:\")\n",
    "for agent_id, history in results['agent_histories'].items():\n",
    "    initial_health = history[0]['health']\n",
    "    final_health = history[-1]['health']\n",
    "    health_change = final_health - initial_health\n",
    "    \n",
    "    initial_morale = history[0]['morale']\n",
    "    final_morale = history[-1]['morale']\n",
    "    morale_change = final_morale - initial_morale\n",
    "    \n",
    "    print(f\"  {agent_id}:\")\n",
    "    print(f\"    Health: {initial_health:.1f} â†’ {final_health:.1f} ({health_change:+.1f})\")\n",
    "    print(f\"    Morale: {initial_morale:.1f} â†’ {final_morale:.1f} ({morale_change:+.1f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffd94bb",
   "metadata": {},
   "source": [
    "## 5. Q-Learning Training\n",
    "\n",
    "Train agents using reinforcement learning to improve their decision-making:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9042bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from softkill9000.environments.environment import QLearningTrainer\n",
    "from softkill9000.config.models import QLearningConfig\n",
    "\n",
    "# Configure Q-learning\n",
    "qlearning_config = QLearningConfig(\n",
    "    learning_rate=0.1,\n",
    "    discount_factor=0.95,\n",
    "    exploration_rate=0.2,\n",
    "    min_exploration_rate=0.01,\n",
    "    exploration_decay=0.995\n",
    ")\n",
    "\n",
    "# Create fresh agents for training\n",
    "training_agents = [\n",
    "    Agent(agent_id=\"Scout-1\", role=\"scout\"),\n",
    "    Agent(agent_id=\"Medic-1\", role=\"medic\"),\n",
    "    Agent(agent_id=\"Assault-1\", role=\"assault\")\n",
    "]\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = QLearningTrainer(config=qlearning_config)\n",
    "\n",
    "# Training loop\n",
    "print(\"ðŸŽ“ Training agents with Q-Learning...\\n\")\n",
    "episodes = 50\n",
    "rewards_history = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    # Create new scenario for each episode\n",
    "    training_scenario = CosmicScenario.generate_random(\n",
    "        width=100.0,\n",
    "        height=100.0,\n",
    "        num_objectives=3,\n",
    "        num_obstacles=5\n",
    "    )\n",
    "    \n",
    "    # Run training episode\n",
    "    sim = MissionSimulator(\n",
    "        agents=training_agents,\n",
    "        scenario=training_scenario,\n",
    "        timesteps=20\n",
    "    )\n",
    "    \n",
    "    results = sim.run_simulation()\n",
    "    episode_reward = results['total_reward']\n",
    "    rewards_history.append(episode_reward)\n",
    "    \n",
    "    # Train agents based on results\n",
    "    for agent in training_agents:\n",
    "        # Simplified training (in practice, you'd pass state-action-reward tuples)\n",
    "        state = hash((agent.position.x, agent.position.y, agent.stats.health))\n",
    "        action = agent.last_action if hasattr(agent, 'last_action') else 'move_forward'\n",
    "        next_state = hash((agent.position.x + 1, agent.position.y, agent.stats.health))\n",
    "        \n",
    "        trainer.train_agent(\n",
    "            agent=agent,\n",
    "            state=state,\n",
    "            action=action,\n",
    "            reward=episode_reward / len(training_agents),\n",
    "            next_state=next_state\n",
    "        )\n",
    "    \n",
    "    if (episode + 1) % 10 == 0:\n",
    "        avg_reward = np.mean(rewards_history[-10:])\n",
    "        print(f\"Episode {episode + 1}/{episodes} - Avg Reward (last 10): {avg_reward:.2f}\")\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rewards_history, alpha=0.6, label='Episode Reward')\n",
    "plt.plot(np.convolve(rewards_history, np.ones(10)/10, mode='valid'), \n",
    "         linewidth=2, label='Moving Average (10)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Q-Learning Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… Training completed!\")\n",
    "print(f\"Final exploration rate: {trainer.config.exploration_rate:.3f}\")\n",
    "print(f\"Average reward (last 10 episodes): {np.mean(rewards_history[-10:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f57db",
   "metadata": {},
   "source": [
    "## 6. Custom Configuration\n",
    "\n",
    "Use YAML configuration for advanced customization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d89b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from softkill9000.config.models import load_config_from_yaml\n",
    "import yaml\n",
    "\n",
    "# Create custom configuration\n",
    "custom_config = \"\"\"\n",
    "simulation:\n",
    "  timesteps: 30\n",
    "  random_seed: 12345\n",
    "  enable_visualization: true\n",
    "  log_level: INFO\n",
    "\n",
    "agents:\n",
    "  - agent_id: \"Alpha\"\n",
    "    role: \"scout\"\n",
    "    initial_health: 100.0\n",
    "    initial_morale: 100.0\n",
    "    speed_multiplier: 1.5\n",
    "  \n",
    "  - agent_id: \"Bravo\"\n",
    "    role: \"assault\"\n",
    "    initial_health: 120.0\n",
    "    initial_morale: 90.0\n",
    "    speed_multiplier: 1.0\n",
    "  \n",
    "  - agent_id: \"Charlie\"\n",
    "    role: \"medic\"\n",
    "    initial_health: 80.0\n",
    "    initial_morale: 100.0\n",
    "    speed_multiplier: 1.2\n",
    "\n",
    "mission:\n",
    "  width: 150.0\n",
    "  height: 150.0\n",
    "  num_objectives: 5\n",
    "  num_obstacles: 8\n",
    "  difficulty: \"hard\"\n",
    "\n",
    "qlearning:\n",
    "  learning_rate: 0.15\n",
    "  discount_factor: 0.98\n",
    "  exploration_rate: 0.3\n",
    "  min_exploration_rate: 0.05\n",
    "  exploration_decay: 0.99\n",
    "\"\"\"\n",
    "\n",
    "# Save to temporary file\n",
    "with open('/tmp/custom_config.yaml', 'w') as f:\n",
    "    f.write(custom_config)\n",
    "\n",
    "# Load configuration\n",
    "config = load_config_from_yaml('/tmp/custom_config.yaml')\n",
    "\n",
    "print(\"âš™ï¸  Custom Configuration Loaded:\")\n",
    "print(f\"  Simulation timesteps: {config.simulation.timesteps}\")\n",
    "print(f\"  Number of agents: {len(config.agents)}\")\n",
    "print(f\"  Mission size: {config.mission.width}x{config.mission.height}\")\n",
    "print(f\"  Difficulty: {config.mission.difficulty}\")\n",
    "\n",
    "# Create agents from config\n",
    "custom_agents = [Agent(**agent.model_dump()) for agent in config.agents]\n",
    "\n",
    "# Create scenario from config\n",
    "custom_scenario = CosmicScenario.generate_random(\n",
    "    width=config.mission.width,\n",
    "    height=config.mission.height,\n",
    "    num_objectives=config.mission.num_objectives,\n",
    "    num_obstacles=config.mission.num_obstacles\n",
    ")\n",
    "\n",
    "# Run custom simulation\n",
    "custom_sim = MissionSimulator(\n",
    "    agents=custom_agents,\n",
    "    scenario=custom_scenario,\n",
    "    timesteps=config.simulation.timesteps\n",
    ")\n",
    "\n",
    "print(\"\\nðŸš€ Running custom simulation...\")\n",
    "custom_results = custom_sim.run_simulation()\n",
    "\n",
    "print(\"\\nðŸ“Š Custom Simulation Results:\")\n",
    "print(f\"  Total Reward: {custom_results['total_reward']:.2f}\")\n",
    "print(f\"  Objectives: {custom_results['objectives_completed']}\")\n",
    "print(f\"  Time: {custom_results['completion_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79203dab",
   "metadata": {},
   "source": [
    "## 7. Compare Multiple Scenarios\n",
    "\n",
    "Run multiple simulations and compare results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61659257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple scenarios\n",
    "print(\"ðŸ”„ Running multiple scenarios...\\n\")\n",
    "\n",
    "scenarios_data = []\n",
    "num_runs = 10\n",
    "\n",
    "for run in range(num_runs):\n",
    "    # Create new agents and scenario for each run\n",
    "    agents = [\n",
    "        Agent(agent_id=f\"Agent-{i}\", role=[\"scout\", \"medic\", \"assault\"][i % 3])\n",
    "        for i in range(3)\n",
    "    ]\n",
    "    \n",
    "    scenario = CosmicScenario.generate_random(\n",
    "        width=100.0,\n",
    "        height=100.0,\n",
    "        num_objectives=3,\n",
    "        num_obstacles=5\n",
    "    )\n",
    "    \n",
    "    sim = MissionSimulator(agents=agents, scenario=scenario, timesteps=20)\n",
    "    results = sim.run_simulation()\n",
    "    \n",
    "    scenarios_data.append({\n",
    "        'run': run + 1,\n",
    "        'reward': results['total_reward'],\n",
    "        'objectives': results['objectives_completed'],\n",
    "        'health': results['average_health'],\n",
    "        'time': results['completion_time']\n",
    "    })\n",
    "\n",
    "# Analyze results\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(scenarios_data)\n",
    "\n",
    "print(\"ðŸ“Š Statistical Summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "axes[0, 0].bar(df['run'], df['reward'], color='steelblue')\n",
    "axes[0, 0].axhline(df['reward'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0, 0].set_xlabel('Run')\n",
    "axes[0, 0].set_ylabel('Total Reward')\n",
    "axes[0, 0].set_title('Reward by Scenario')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].bar(df['run'], df['objectives'], color='green')\n",
    "axes[0, 1].axhline(df['objectives'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0, 1].set_xlabel('Run')\n",
    "axes[0, 1].set_ylabel('Objectives Completed')\n",
    "axes[0, 1].set_title('Objectives by Scenario')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].bar(df['run'], df['health'], color='orange')\n",
    "axes[1, 0].axhline(df['health'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[1, 0].set_xlabel('Run')\n",
    "axes[1, 0].set_ylabel('Average Health (%)')\n",
    "axes[1, 0].set_title('Final Health by Scenario')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].bar(df['run'], df['time'], color='purple')\n",
    "axes[1, 1].axhline(df['time'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[1, 1].set_xlabel('Run')\n",
    "axes[1, 1].set_ylabel('Time (seconds)')\n",
    "axes[1, 1].set_title('Completion Time by Scenario')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… Completed {num_runs} scenario runs\")\n",
    "print(f\"Average reward: {df['reward'].mean():.2f} Â± {df['reward'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3f89e3",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Next Steps\n",
    "\n",
    "Explore more features:\n",
    "\n",
    "1. **REST API**: Deploy the FastAPI server for remote access\n",
    "2. **Custom Agents**: Create agents with specialized behaviors\n",
    "3. **Advanced Scenarios**: Design complex mission environments\n",
    "4. **Extended Training**: Train agents for more episodes to see performance improvements\n",
    "5. **Visualization**: Create custom plots and animations\n",
    "\n",
    "### Documentation\n",
    "\n",
    "- [Architecture Guide](https://github.com/BkAsDrP/Softkill9000/blob/main/docs/architecture.md)\n",
    "- [API Reference](https://github.com/BkAsDrP/Softkill9000/blob/main/docs/api_reference.md)\n",
    "- [User Guide](https://github.com/BkAsDrP/Softkill9000/blob/main/docs/user_guide.md)\n",
    "- [Deployment Guide](https://github.com/BkAsDrP/Softkill9000/blob/main/docs/deployment.md)\n",
    "\n",
    "### Repository\n",
    "\n",
    "- GitHub: [BkAsDrP/Softkill9000](https://github.com/BkAsDrP/Softkill9000)\n",
    "- Issues: [Report bugs or request features](https://github.com/BkAsDrP/Softkill9000/issues)\n",
    "- License: MIT\n",
    "\n",
    "---\n",
    "\n",
    "**Happy simulating! ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
