{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b6c9e5",
   "metadata": {},
   "source": [
    "# SOFTKILL-9000: Multi-Agent Motion Capture Simulation\n",
    "\n",
    "This notebook demonstrates how to run SOFTKILL-9000 in Google Colab.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/BkAsDrP/Softkill9000/blob/main/examples/run_in_colab.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "SOFTKILL-9000 is a multi-agent reinforcement learning system for motion capture simulation. This notebook will:\n",
    "\n",
    "1. Install the package from GitHub\n",
    "2. Run a basic simulation\n",
    "3. Visualize agent trajectories\n",
    "4. Demonstrate Q-learning training\n",
    "5. Show advanced customization options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7687a26b",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "Install SOFTKILL-9000 directly from the GitHub repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57b50f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "‚úÖ SOFTKILL-9000 package installed!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install the package from GitHub\n",
    "!pip install git+https://github.com/BkAsDrP/Softkill9000.git -q\n",
    "print(\"‚úÖ SOFTKILL-9000 package installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b96739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "‚úÖ NumPy 2.x installed\n",
      "\n",
      "‚ö†Ô∏è  IMPORTANT: If using this notebook locally:\n",
      "   1. Restart your Jupyter kernel to apply NumPy upgrade\n",
      "   2. In VS Code: Click the 'Restart' button in the kernel toolbar\n",
      "   3. Then continue with the verification cell below\n",
      "\n",
      "üí° Note: Auto-restart only works in Google Colab\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Upgrade NumPy to resolve dependency conflicts\n",
    "# Colab comes with NumPy 1.x, but softkill9000 needs NumPy 2.0.x for modern ML libraries\n",
    "# Pin to <2.1.0 for compatibility with numba, tensorflow, opencv, and cupy\n",
    "!pip install --upgrade \"numpy>=2.0.0,<2.1.0\" -q\n",
    "\n",
    "print(\"‚úÖ NumPy 2.x installed\")\n",
    "print(\"\")\n",
    "\n",
    "# Detect if running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Auto-restart only in Colab\n",
    "    print(\"üîÑ Auto-restarting Colab runtime in 3 seconds...\")\n",
    "    print(\"   (This prevents binary incompatibility errors)\")\n",
    "    \n",
    "    import time\n",
    "    import os\n",
    "    time.sleep(3)\n",
    "    os.kill(os.getpid(), 9)\n",
    "else:\n",
    "    # Local environment - just show a message\n",
    "    print(\"‚ö†Ô∏è  IMPORTANT: If using this notebook locally:\")\n",
    "    print(\"   1. Restart your Jupyter kernel to apply NumPy upgrade\")\n",
    "    print(\"   2. In VS Code: Click the 'Restart' button in the kernel toolbar\")\n",
    "    print(\"   3. Then continue with the verification cell below\")\n",
    "    print(\"\")\n",
    "    print(\"üí° Note: Auto-restart only works in Google Colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30de9cf",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Restart Required\n",
    "\n",
    "**If running in Google Colab**: The runtime will automatically restart after the cell above.\n",
    "\n",
    "**If running locally** (VS Code, JupyterLab, etc.): Manually restart your kernel now:\n",
    "- VS Code: Click the **Restart** button in the kernel toolbar\n",
    "- JupyterLab: Click **Kernel ‚Üí Restart Kernel**\n",
    "- Jupyter Notebook: Click **Kernel ‚Üí Restart**\n",
    "\n",
    "After restarting, continue with the verification cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e279bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SOFTKILL-9000 v1.0.0 ready!\n",
      "‚úÖ NumPy 2.3.4 (compatible with JAX, OpenCV, PyTensor)\n",
      "\n",
      "üéâ Installation complete and verified! Continue with examples below.\n"
     ]
    }
   ],
   "source": [
    "# Verification (Run this AFTER runtime restart)\n",
    "try:\n",
    "    import softkill9000\n",
    "    import numpy as np\n",
    "    \n",
    "    print(f\"‚úÖ SOFTKILL-9000 v{softkill9000.__version__} ready!\")\n",
    "    print(f\"‚úÖ NumPy {np.__version__} (compatible with JAX, OpenCV, PyTensor)\")\n",
    "    \n",
    "    # Test that numpy actually works (catches binary incompatibility)\n",
    "    test_array = np.random.rand(5)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"üéâ Installation complete and verified! Continue with examples below.\")\n",
    "    \n",
    "except ValueError as e:\n",
    "    if \"numpy.dtype size changed\" in str(e):\n",
    "        print(\"‚ùå Binary incompatibility detected!\")\n",
    "        print(\"\")\n",
    "        print(\"SOLUTION: Manually restart the runtime:\")\n",
    "        print(\"   1. Click 'Runtime ‚Üí Restart runtime' in the menu\")\n",
    "        print(\"   2. Re-run this cell after restart\")\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ec9d7b",
   "metadata": {},
   "source": [
    "## 2. Basic Simulation\n",
    "\n",
    "Let's run a basic simulation with default configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59852e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Agent.__init__() got an unexpected keyword argument 'agent_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m np.random.seed(\u001b[32m42\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Create agents\u001b[39;00m\n\u001b[32m     10\u001b[39m agents = [\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLongsight\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrole\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscout\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[32m     12\u001b[39m     Agent(agent_id=\u001b[33m\"\u001b[39m\u001b[33mLifebinder\u001b[39m\u001b[33m\"\u001b[39m, role=\u001b[33m\"\u001b[39m\u001b[33mmedic\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     13\u001b[39m     Agent(agent_id=\u001b[33m\"\u001b[39m\u001b[33mSpecter\u001b[39m\u001b[33m\"\u001b[39m, role=\u001b[33m\"\u001b[39m\u001b[33massault\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m ]\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Create scenario\u001b[39;00m\n\u001b[32m     17\u001b[39m scenario = CosmicScenario.generate_random(\n\u001b[32m     18\u001b[39m     width=\u001b[32m100.0\u001b[39m,\n\u001b[32m     19\u001b[39m     height=\u001b[32m100.0\u001b[39m,\n\u001b[32m     20\u001b[39m     num_objectives=\u001b[32m3\u001b[39m,\n\u001b[32m     21\u001b[39m     num_obstacles=\u001b[32m5\u001b[39m\n\u001b[32m     22\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: Agent.__init__() got an unexpected keyword argument 'agent_id'"
     ]
    }
   ],
   "source": [
    "from softkill9000.simulator import MissionSimulator\n",
    "from softkill9000.agents.agent import Agent, AgentStats\n",
    "from softkill9000.environments.environment import CosmicScenario\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create agents with proper parameters\n",
    "agents = [\n",
    "    Agent(\n",
    "        role=\"Longsight\",\n",
    "        description=\"Scout with enhanced mobility and intelligence\",\n",
    "        species=\"Vyr'khai\",\n",
    "        stats=AgentStats(intelligence=80, mobility=85, tactical=75)\n",
    "    ),\n",
    "    Agent(\n",
    "        role=\"Lifebinder\",\n",
    "        description=\"Medic with high empathy and support abilities\",\n",
    "        species=\"Lumenari\",\n",
    "        stats=AgentStats(empathy=90, intelligence=75, tactical=65)\n",
    "    ),\n",
    "    Agent(\n",
    "        role=\"Specter\",\n",
    "        description=\"Assault specialist with superior strength\",\n",
    "        species=\"Zek'thar\",\n",
    "        stats=AgentStats(strength=95, tactical=80, mobility=70)\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create scenario\n",
    "scenario = CosmicScenario.generate_random(\n",
    "    width=100.0,\n",
    "    height=100.0,\n",
    "    num_objectives=3,\n",
    "    num_obstacles=5\n",
    ")\n",
    "\n",
    "# Initialize simulator\n",
    "simulator = MissionSimulator(\n",
    "    agents=agents,\n",
    "    scenario=scenario,\n",
    "    timesteps=20\n",
    ")\n",
    "\n",
    "# Run simulation\n",
    "print(\"üöÄ Starting simulation...\\n\")\n",
    "results = simulator.run_simulation()\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä SIMULATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Reward: {results['total_reward']:.2f}\")\n",
    "print(f\"Objectives Completed: {results['objectives_completed']}\")\n",
    "print(f\"Average Health: {results['average_health']:.1f}%\")\n",
    "print(f\"Final Morale: {results['final_morale']:.1f}%\")\n",
    "print(f\"Completion Time: {results['completion_time']:.2f}s\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1804a461",
   "metadata": {},
   "source": [
    "## 3. Visualize Agent Trajectories\n",
    "\n",
    "Plot the paths taken by each agent during the mission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d4b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from softkill9000.visualization import plot_agent_trajectories\n",
    "\n",
    "# Plot trajectories\n",
    "fig = plot_agent_trajectories(results['agent_histories'], scenario)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìç Agent Movement Summary:\")\n",
    "for agent_id, history in results['agent_histories'].items():\n",
    "    positions = [(h['position']['x'], h['position']['y']) for h in history]\n",
    "    start = positions[0]\n",
    "    end = positions[-1]\n",
    "    distance = sum(\n",
    "        np.sqrt((positions[i+1][0] - positions[i][0])**2 + \n",
    "                (positions[i+1][1] - positions[i][1])**2)\n",
    "        for i in range(len(positions)-1)\n",
    "    )\n",
    "    print(f\"  {agent_id}: {distance:.1f} units traveled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbd28fe",
   "metadata": {},
   "source": [
    "## 4. Performance Metrics Over Time\n",
    "\n",
    "Visualize how agent stats evolved during the simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31ffac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from softkill9000.visualization import plot_performance_metrics\n",
    "\n",
    "# Plot performance metrics\n",
    "fig = plot_performance_metrics(results['agent_histories'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà Performance Analysis:\")\n",
    "for agent_id, history in results['agent_histories'].items():\n",
    "    initial_health = history[0]['health']\n",
    "    final_health = history[-1]['health']\n",
    "    health_change = final_health - initial_health\n",
    "    \n",
    "    initial_morale = history[0]['morale']\n",
    "    final_morale = history[-1]['morale']\n",
    "    morale_change = final_morale - initial_morale\n",
    "    \n",
    "    print(f\"  {agent_id}:\")\n",
    "    print(f\"    Health: {initial_health:.1f} ‚Üí {final_health:.1f} ({health_change:+.1f})\")\n",
    "    print(f\"    Morale: {initial_morale:.1f} ‚Üí {final_morale:.1f} ({morale_change:+.1f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffd94bb",
   "metadata": {},
   "source": [
    "## 5. Q-Learning Training\n",
    "\n",
    "Train agents using reinforcement learning to improve their decision-making:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9042bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from softkill9000.environments.environment import QLearningTrainer\n",
    "from softkill9000.config.models import QLearningConfig\n",
    "\n",
    "# Configure Q-learning\n",
    "qlearning_config = QLearningConfig(\n",
    "    learning_rate=0.1,\n",
    "    discount_factor=0.95,\n",
    "    exploration_rate=0.2,\n",
    "    min_exploration_rate=0.01,\n",
    "    exploration_decay=0.995\n",
    ")\n",
    "\n",
    "# Create fresh agents for training\n",
    "training_agents = [\n",
    "    Agent(\n",
    "        role=\"Longsight\",\n",
    "        description=\"Scout trainee\",\n",
    "        species=\"Vyr'khai\",\n",
    "        stats=AgentStats(intelligence=75, mobility=80)\n",
    "    ),\n",
    "    Agent(\n",
    "        role=\"Lifebinder\",\n",
    "        description=\"Medic trainee\",\n",
    "        species=\"Lumenari\",\n",
    "        stats=AgentStats(empathy=85, intelligence=70)\n",
    "    ),\n",
    "    Agent(\n",
    "        role=\"Specter\",\n",
    "        description=\"Assault trainee\",\n",
    "        species=\"Zek'thar\",\n",
    "        stats=AgentStats(strength=90, tactical=75)\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = QLearningTrainer(config=qlearning_config)\n",
    "\n",
    "# Training loop\n",
    "print(\"üéì Training agents with Q-Learning...\\n\")\n",
    "episodes = 50\n",
    "rewards_history = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    # Create new scenario for each episode\n",
    "    training_scenario = CosmicScenario.generate_random(\n",
    "        width=100.0,\n",
    "        height=100.0,\n",
    "        num_objectives=3,\n",
    "        num_obstacles=5\n",
    "    )\n",
    "    \n",
    "    # Run training episode\n",
    "    sim = MissionSimulator(\n",
    "        agents=training_agents,\n",
    "        scenario=training_scenario,\n",
    "        timesteps=20\n",
    "    )\n",
    "    \n",
    "    results = sim.run_simulation()\n",
    "    episode_reward = results['total_reward']\n",
    "    rewards_history.append(episode_reward)\n",
    "    \n",
    "    # Train agents based on results\n",
    "    for agent in training_agents:\n",
    "        # Simplified training (in practice, you'd pass state-action-reward tuples)\n",
    "        state = hash((agent.position[0], agent.position[1], agent.stats.strength))\n",
    "        action = 'advance'  # Simplified action\n",
    "        next_state = hash((agent.position[0] + 1, agent.position[1], agent.stats.strength))\n",
    "        \n",
    "        trainer.train_agent(\n",
    "            agent=agent,\n",
    "            state=state,\n",
    "            action=action,\n",
    "            reward=episode_reward / len(training_agents),\n",
    "            next_state=next_state\n",
    "        )\n",
    "    \n",
    "    if (episode + 1) % 10 == 0:\n",
    "        avg_reward = np.mean(rewards_history[-10:])\n",
    "        print(f\"Episode {episode + 1}/{episodes} - Avg Reward (last 10): {avg_reward:.2f}\")\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rewards_history, alpha=0.6, label='Episode Reward')\n",
    "plt.plot(np.convolve(rewards_history, np.ones(10)/10, mode='valid'), \n",
    "         linewidth=2, label='Moving Average (10)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Q-Learning Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed!\")\n",
    "print(f\"Final exploration rate: {trainer.config.exploration_rate:.3f}\")\n",
    "print(f\"Average reward (last 10 episodes): {np.mean(rewards_history[-10:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f57db",
   "metadata": {},
   "source": [
    "## 6. Custom Configuration\n",
    "\n",
    "Use YAML configuration for advanced customization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d89b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# For this example, we'll create agents manually since the config structure\n",
    "# doesn't directly map to the Agent dataclass\n",
    "print(\"‚öôÔ∏è  Creating custom agent configurations...\\n\")\n",
    "\n",
    "# Define custom agents with different stats\n",
    "custom_agents = [\n",
    "    Agent(\n",
    "        role=\"Longsight\",\n",
    "        description=\"Alpha scout with enhanced mobility\",\n",
    "        species=\"Vyr'khai\",\n",
    "        stats=AgentStats(intelligence=85, mobility=90, tactical=80, strength=55, empathy=60)\n",
    "    ),\n",
    "    Agent(\n",
    "        role=\"Specter\",\n",
    "        description=\"Bravo assault with superior firepower\",\n",
    "        species=\"Zek'thar\",\n",
    "        stats=AgentStats(strength=100, tactical=85, mobility=75, intelligence=65, empathy=50)\n",
    "    ),\n",
    "    Agent(\n",
    "        role=\"Lifebinder\",\n",
    "        description=\"Charlie medic with maximum support\",\n",
    "        species=\"Lumenari\",\n",
    "        stats=AgentStats(empathy=95, intelligence=80, tactical=70, strength=45, mobility=65)\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created {len(custom_agents)} custom agents:\")\n",
    "for agent in custom_agents:\n",
    "    print(f\"   - {agent.role} ({agent.species}): {agent.description}\")\n",
    "\n",
    "# Create custom scenario (larger and more challenging)\n",
    "custom_scenario = CosmicScenario.generate_random(\n",
    "    width=150.0,\n",
    "    height=150.0,\n",
    "    num_objectives=5,\n",
    "    num_obstacles=8\n",
    ")\n",
    "\n",
    "print(f\"\\nüó∫Ô∏è  Custom Scenario:\")\n",
    "print(f\"   Size: 150x150 units\")\n",
    "print(f\"   Objectives: 5\")\n",
    "print(f\"   Obstacles: 8\")\n",
    "\n",
    "# Run custom simulation with more timesteps\n",
    "custom_sim = MissionSimulator(\n",
    "    agents=custom_agents,\n",
    "    scenario=custom_scenario,\n",
    "    timesteps=30  # Longer mission\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Running custom simulation (30 timesteps)...\")\n",
    "custom_results = custom_sim.run_simulation()\n",
    "\n",
    "print(\"\\nüìä Custom Simulation Results:\")\n",
    "print(f\"  Total Reward: {custom_results['total_reward']:.2f}\")\n",
    "print(f\"  Objectives Completed: {custom_results['objectives_completed']}/5\")\n",
    "print(f\"  Average Health: {custom_results['average_health']:.1f}%\")\n",
    "print(f\"  Final Morale: {custom_results['final_morale']:.1f}%\")\n",
    "print(f\"  Completion Time: {custom_results['completion_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79203dab",
   "metadata": {},
   "source": [
    "## 7. Compare Multiple Scenarios\n",
    "\n",
    "Run multiple simulations and compare results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61659257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple scenarios\n",
    "print(\"üîÑ Running multiple scenarios...\\n\")\n",
    "\n",
    "scenarios_data = []\n",
    "num_runs = 10\n",
    "\n",
    "role_config = [\n",
    "    (\"Longsight\", \"Scout\", \"Vyr'khai\", AgentStats(intelligence=75, mobility=80)),\n",
    "    (\"Lifebinder\", \"Medic\", \"Lumenari\", AgentStats(empathy=85, intelligence=70)),\n",
    "    (\"Specter\", \"Assault\", \"Zek'thar\", AgentStats(strength=90, tactical=75))\n",
    "]\n",
    "\n",
    "for run in range(num_runs):\n",
    "    # Create new agents and scenario for each run\n",
    "    agents = [\n",
    "        Agent(\n",
    "            role=role_config[i % 3][0],\n",
    "            description=f\"{role_config[i % 3][1]} agent #{run}-{i}\",\n",
    "            species=role_config[i % 3][2],\n",
    "            stats=role_config[i % 3][3]\n",
    "        )\n",
    "        for i in range(3)\n",
    "    ]\n",
    "    \n",
    "    scenario = CosmicScenario.generate_random(\n",
    "        width=100.0,\n",
    "        height=100.0,\n",
    "        num_objectives=3,\n",
    "        num_obstacles=5\n",
    "    )\n",
    "    \n",
    "    sim = MissionSimulator(agents=agents, scenario=scenario, timesteps=20)\n",
    "    results = sim.run_simulation()\n",
    "    \n",
    "    scenarios_data.append({\n",
    "        'run': run + 1,\n",
    "        'reward': results['total_reward'],\n",
    "        'objectives': results['objectives_completed'],\n",
    "        'health': results['average_health'],\n",
    "        'time': results['completion_time']\n",
    "    })\n",
    "\n",
    "# Analyze results\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(scenarios_data)\n",
    "\n",
    "print(\"üìä Statistical Summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "axes[0, 0].bar(df['run'], df['reward'], color='steelblue')\n",
    "axes[0, 0].axhline(df['reward'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0, 0].set_xlabel('Run')\n",
    "axes[0, 0].set_ylabel('Total Reward')\n",
    "axes[0, 0].set_title('Reward by Scenario')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].bar(df['run'], df['objectives'], color='green')\n",
    "axes[0, 1].axhline(df['objectives'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0, 1].set_xlabel('Run')\n",
    "axes[0, 1].set_ylabel('Objectives Completed')\n",
    "axes[0, 1].set_title('Objectives by Scenario')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].bar(df['run'], df['health'], color='orange')\n",
    "axes[1, 0].axhline(df['health'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[1, 0].set_xlabel('Run')\n",
    "axes[1, 0].set_ylabel('Average Health (%)')\n",
    "axes[1, 0].set_title('Final Health by Scenario')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].bar(df['run'], df['time'], color='purple')\n",
    "axes[1, 1].axhline(df['time'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[1, 1].set_xlabel('Run')\n",
    "axes[1, 1].set_ylabel('Time (seconds)')\n",
    "axes[1, 1].set_title('Completion Time by Scenario')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Completed {num_runs} scenario runs\")\n",
    "print(f\"Average reward: {df['reward'].mean():.2f} ¬± {df['reward'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3f89e3",
   "metadata": {},
   "source": [
    "## üéØ Next Steps\n",
    "\n",
    "Explore more features:\n",
    "\n",
    "1. **REST API**: Deploy the FastAPI server for remote access\n",
    "2. **Custom Agents**: Create agents with specialized behaviors\n",
    "3. **Advanced Scenarios**: Design complex mission environments\n",
    "4. **Extended Training**: Train agents for more episodes to see performance improvements\n",
    "5. **Visualization**: Create custom plots and animations\n",
    "\n",
    "### Documentation\n",
    "\n",
    "- [Architecture Guide](https://github.com/BkAsDrP/Softkill9000/blob/main/docs/architecture.md)\n",
    "- [API Reference](https://github.com/BkAsDrP/Softkill9000/blob/main/docs/api_reference.md)\n",
    "- [User Guide](https://github.com/BkAsDrP/Softkill9000/blob/main/docs/user_guide.md)\n",
    "- [Deployment Guide](https://github.com/BkAsDrP/Softkill9000/blob/main/docs/deployment.md)\n",
    "\n",
    "### Repository\n",
    "\n",
    "- GitHub: [BkAsDrP/Softkill9000](https://github.com/BkAsDrP/Softkill9000)\n",
    "- Issues: [Report bugs or request features](https://github.com/BkAsDrP/Softkill9000/issues)\n",
    "- License: MIT\n",
    "\n",
    "---\n",
    "\n",
    "**Happy simulating! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
